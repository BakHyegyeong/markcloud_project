/**
 * Options for IterableMapper
 */
export interface IterableMapperOptions {
    /**
     * Maximum number of concurrent invocations of `mapper` to run at once.
     *
     * The number of concurrent invocations is dynamically adjusted based on the `maxUnread` limit:
     * - If there are no unread items and `maxUnread` is 10 with `concurrency` of 4, all 4 mappers can run.
     * - If there are already 8 unread items in the queue, only 2 mappers will run to avoid exceeding
     *   the `maxUnread` limit of 10.
     * - If there are 10 unread items, no mappers will run until an item is consumed from the queue.
     *
     * This ensures efficient processing while maintaining backpressure through the `maxUnread` limit.
     *
     * Setting `concurrency` to 1 enables serial processing, preserving the order of items
     * while still benefiting from the backpressure mechanism.
     *
     * Must be an integer from 1 and up or `Infinity`, and must be <= `maxUnread`.
     *
     * @default 4
     */
    readonly concurrency?: number;
    /**
     * Maximum number of unread items allowed to accumulate before applying backpressure.
     *
     * This parameter is crucial for controlling memory usage and system load by:
     * 1. Limiting the number of processed but unread items in the queue
     * 2. Automatically pausing mapper execution when the limit is reached
     * 3. Resuming processing when items are consumed, maintaining optimal throughput
     *
     * For example, when reading from a slow database:
     * - With maxUnread=10, only 10 items will be fetched before the consumer reads them
     * - Additional items won't be fetched until the consumer reads existing items
     * - This prevents runaway memory usage for items that cannot be processed quickly enough
     *
     * Must be an integer from 1 and up or `Infinity`, and must be >= `concurrency`.
     * It is not typical to set this value to `Infinity`, but rather to a value such as 1 to 10.
     *
     * @default 8
     */
    readonly maxUnread?: number;
    /**
     * When set to `false`, instead of stopping when a promise rejects, it will wait for all
     * the promises to settle and then reject with an
     * [aggregated error](https://github.com/sindresorhus/aggregate-error) containing all the
     * errors from the rejected promises.
     *
     * @default true
     */
    readonly stopOnMapperError?: boolean;
}
/**
 * Function which is called for every item in `input`. Expected to return a `Promise` or value.
 *
 * @template Element - Source element type
 * @template NewElement - Element type returned by the mapper
 * @param element - Iterated element
 * @param index - Index of the element in the source array
 */
export type Mapper<Element = unknown, NewElement = unknown> = (element: Element, index: number) => NewElement | Promise<NewElement>;
/**
 * Iterates over a source iterable / generator with specified `concurrency`,
 * calling the `mapper` on each iterated item, and storing the
 * `mapper` result in a queue of `maxUnread` size, before
 * being iterated / read by the caller.
 *
 * @remarks
 *
 * ### Typical Use Case
 * - Prefetching items from an async I/O source
 * - In the simple sequential (`concurrency: 1`) case, allows items to be prefetched async, preserving order, while caller processes an item
 * - Can allow parallel prefetches for sources that allow for out of order reads (`concurrency:  2+`)
 * - Prevents the producer from racing ahead of the consumer if `maxUnread` is reached
 *
 * ### Error Handling
 *   The mapper should ideally handle all errors internally to enable error handling
 *   closest to where they occur. However, if errors do escape the mapper:
 *
 *   When `stopOnMapperError` is true (default):
 *   - First error immediately stops processing
 *   - Error is thrown from the `AsyncIterator`'s next() call
 *
 *   When `stopOnMapperError` is false:
 *   - Processing continues despite errors
 *   - All errors are collected and thrown together
 *   - Errors are thrown as `AggregateError` after all items complete
 *
 * ### Usage
 * - Items are exposed to the `mapper` via an iterator or async iterator (this includes generator and async generator functions)
 * - IMPORTANT: `mapper` method not be invoked when `maxUnread` is reached, until items are consumed
 * - The iterable will set `done` when the `input` has indicated `done` and all `mapper` promises have resolved
 *
 * @example
 *
 * ### Typical Processing Loop without `IterableMapper`
 *
 * ```typescript
 * const source = new SomeSource();
 * const sourceIds = [1, 2,... 1000];
 * const sink = new SomeSink();
 * for (const sourceId of sourceIds) {
 *   const item = await source.read(sourceId);     // takes 300 ms of I/O wait, no CPU
 *   const outputItem = doSomeOperation(item);     // takes 20 ms of CPU
 *   await sink.write(outputItem);                 // takes 500 ms of I/O wait, no CPU
 * }
 * ```
 *
 * Each iteration takes 820ms total, but we waste time waiting for I/O.
 * We could prefetch the next read (300ms) while processing (20ms) and writing (500ms),
 * without changing the order of reads or writes.
 *
 * @example
 *
 * ### Using `IterableMapper` as Prefetcher with Blocking Sequential Writes
 *
 * `concurrency: 1` on the prefetcher preserves the order of the reads and and writes are sequential and blocking (unchanged).
 *
 * ```typescript
 * const source = new SomeSource();
 * const sourceIds = [1, 2,... 1000];
 * // Pre-reads up to 8 items serially and releases in sequential order
 * const sourcePrefetcher = new IterableMapper(sourceIds,
 *   async (sourceId) => source.read(sourceId),
 *   { concurrency: 1, maxUnread: 10 }
 * );
 * const sink = new SomeSink();
 * for await (const item of sourcePrefetcher) {    // may not block for fast sources
 *   const outputItem = doSomeOperation(item);     // takes 20 ms of CPU
 *   await sink.write(outputItem);                 // takes 500 ms of I/O wait, no CPU
 * }
 * ```
 *
 * This reduces iteration time to 520ms by overlapping reads with processing/writing.
 *
 * @example
 *
 * ### Using `IterableMapper` as Prefetcher with Background Sequential Writes with `IterableQueueMapperSimple`
 *
 * `concurrency: 1` on the prefetcher preserves the order of the reads.
 * `concurrency: 1` on the flusher preserves the order of the writes, but allows the loop to iterate while last write is completing.
 *
 * ```typescript
 * const source = new SomeSource();
 * const sourceIds = [1, 2,... 1000];
 * const sourcePrefetcher = new IterableMapper(sourceIds,
 *   async (sourceId) => source.read(sourceId),
 *   { concurrency: 1, maxUnread: 10 }
 * );
 * const sink = new SomeSink();
 * const flusher = new IterableQueueMapperSimple(
 *   async (outputItem) => sink.write(outputItem),
 *   { concurrency: 1 }
 * );
 * for await (const item of sourcePrefetcher) {    // may not block for fast sources
 *   const outputItem = doSomeOperation(item);     // takes 20 ms of CPU
 *   await flusher.enqueue(outputItem);            // will periodically block for portion of write time
 * }
 * // Wait for all writes to complete
 * await flusher.onIdle();
 * // Check for errors
 * if (flusher.errors.length > 0) {
 *  // ...
 * }
 * ```
 *
 * This reduces iteration time to about `max((max(readTime, writeTime) - cpuOpTime, cpuOpTime))`
 * by overlapping reads and writes with the CPU processing step.
 * In this contrived example, the loop time is reduced to 500ms - 20ms = 480ms.
 * In cases where the CPU usage time is higher, the impact can be greater.
 *
 * @example
 *
 * ### Using `IterableMapper` as Prefetcher with Out of Order Reads and Background Out of Order Writes with `IterableQueueMapperSimple`
 *
 * For maximum throughput, allow out of order reads and writes with
 * `IterableQueueMapper` (to iterate results with backpressure when too many unread items) or
 * `IterableQueueMapperSimple` (to handle errors at end without custom iteration and applying backpressure to block further enqueues when `concurrency` items are in process):
 *
 * ```typescript
 * const source = new SomeSource();
 * const sourceIds = [1, 2,... 1000];
 * const sourcePrefetcher = new IterableMapper(sourceIds,
 *   async (sourceId) => source.read(sourceId),
 *   { concurrency: 10, maxUnread: 20 }
 * );
 * const sink = new SomeSink();
 * const flusher = new IterableQueueMapperSimple(
 *   async (outputItem) => sink.write(outputItem),
 *   { concurrency: 10 }
 * );
 * for await (const item of sourcePrefetcher) {    // typically will not block
 *   const outputItem = doSomeOperation(item);     // takes 20 ms of CPU
 *   await flusher.enqueue(outputItem);            // typically will not block
 * }
 * // Wait for all writes to complete
 * await flusher.onIdle();
 * // Check for errors
 * if (flusher.errors.length > 0) {
 *  // ...
 * }
 * ```
 *
 * This reduces iteration time to about 20ms by overlapping reads and writes with the CPU processing step.
 * In this contrived (but common) example we would get a 41x improvement in throughput, removing 97.5% of
 * the time to process each item and fully utilizing the CPU time available in the JS event loop.
 *
 * @category Iterable Input
 */
export declare class IterableMapper<Element, NewElement> implements AsyncIterable<NewElement> {
    private _mapper;
    private _options;
    private _unreadQueue;
    private _iterator;
    private readonly _errors;
    private _asyncIterator;
    private _isRejected;
    private _isIterableDone;
    private _activeRunners;
    private _resolvingCount;
    private _currentIndex;
    private _initialRunnersCreated;
    /**
     * Create a new `IterableMapper`
     *
     * @param input Iterated over concurrently, or serially, in the `mapper` function.
     * @param mapper Function called for every item in `input`. Returns a `Promise` or value.
     * @param options IterableMapper options
     *
     * @see {@link IterableQueueMapper} for full class documentation
     */
    constructor(input: AsyncIterable<Element> | Iterable<Element>, mapper: Mapper<Element, NewElement>, options?: IterableMapperOptions);
    [Symbol.asyncIterator](): AsyncIterator<NewElement>;
    /**
     * Used by the iterator returned from [Symbol.asyncIterator]
     * Called every time an item is needed
     *
     * @returns Iterator result
     */
    next(): Promise<IteratorResult<NewElement>>;
    private bubbleUpErrors;
    private startARunnerIfNeeded;
    private startAnotherRunner;
    private areWeDone;
    /**
     * Throw an exception if the wrapped NewElement is an Error
     *
     * @returns Element if no error
     */
    private throwIfError;
    /**
     * Get the next item from the `input` iterable.
     *
     * @remarks
     *
     * This is called up to `concurrency` times in parallel.
     *
     * If the read queue is not full, and there are source items to read,
     * each instance of this will keep calling a new instance of itself
     * that detaches and runs asynchronously (keeping the same number
     * of instances running).
     *
     * If the read queue + runners = max read queue length then the runner
     * will exit and will be restarted when an item is read from the queue.
     */
    private sourceNext;
}
//# sourceMappingURL=iterable-mapper.d.ts.map